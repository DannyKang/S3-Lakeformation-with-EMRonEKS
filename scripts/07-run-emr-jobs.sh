#!/bin/bash

# EMR on EKS Job ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Lake Formation FGAC + Apache Iceberg)
# Lake Formation FGAC 4ê°œ ì—­í• ë³„ ë¶„ì„ Job ì‹¤í–‰
# Data Cells Filter ë°©ì‹ (Hybrid Access Mode ë¶ˆí•„ìš”)
# ì—…ë°ì´íŠ¸: 2025-08-04 - EMR Serverless ë°©ì‹ ì°¸ì¡°í•˜ì—¬ HybridAccessMode ìš”êµ¬ì‚¬í•­ ì œê±°

set -e

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
if [ ! -f ".env" ]; then
    echo "âŒ .env íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € 01-create-s3-bucket.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

source .env

# Lake Formation FGAC ì„¤ì • í™•ì¸
if [ -z "$LF_VIRTUAL_CLUSTER_ID" ]; then
    echo "âŒ Lake Formation FGACê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € ./scripts/05-setup-emr-on-eks.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

# Lake Formation FGAC ì„¤ì • í™•ì¸ (Data Cells Filter ë°©ì‹)
echo "ğŸ” Lake Formation FGAC ì„¤ì • í™•ì¸ ì¤‘..."

# Data Cells Filter ì¡´ì¬ í™•ì¸
FILTER_COUNT=$(aws lakeformation list-data-cells-filter \
    --region $REGION \
    --table "{
        \"CatalogId\": \"${ACCOUNT_ID}\",
        \"DatabaseName\": \"bike_db\",
        \"Name\": \"bike_rental_data\"
    }" \
    --query 'length(DataCellsFilters)' \
    --output text 2>/dev/null || echo "0")

if [ "$FILTER_COUNT" -eq 0 ]; then
    echo "âŒ Lake Formation Data Cells Filterê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "   í•´ê²° ë°©ë²•: ./scripts/04-setup-lakeformation-permissions-iceberg.shë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    exit 1
fi

echo "âœ… Lake Formation FGAC ì„¤ì • í™•ì¸ë¨ (${FILTER_COUNT}ê°œ Data Cells Filter)"

echo "=== EMR on EKS Job ì‹¤í–‰ ì‹œì‘ (Lake Formation FGAC + Apache Iceberg) ==="
echo "LF Virtual Cluster ID: $LF_VIRTUAL_CLUSTER_ID"
echo "Security Configuration: $SECURITY_CONFIG_ID"
echo "Session Tag Value: EMRonEKSEngine"
echo "User Namespace: $USER_NAMESPACE"
echo "Scripts Bucket: s3://$SCRIPTS_BUCKET"
echo "Iceberg Bucket: s3://$ICEBERG_BUCKET_NAME"
echo "Job Templates Directory: ./job-templates/"
echo "Pod Templates Directory: ./pod-templates/"
echo ""

# Job ì„¤ì • (Lake Formation FGAC ì—­í•  ë§¤í•‘)
JOB_CONFIGS=(
    "data-steward:emr-data-steward-sa:$LF_DATA_STEWARD_ROLE:ë°ì´í„° ìŠ¤íŠœì–´ë“œ ì „ì²´ ë°ì´í„° ë¶„ì„ (100,000ê±´)"
    "gangnam-analytics:emr-gangnam-analytics-sa:$LF_GANGNAM_ANALYTICS_ROLE:ê°•ë‚¨êµ¬ ë°ì´í„° ë¶„ì„ (~3,000ê±´)" 
    "operation:emr-operation-sa:$LF_OPERATION_ROLE:ìš´ì˜ ë°ì´í„° ë¶„ì„ (ê°œì¸ì •ë³´ ì œì™¸)"
    "marketing-partner:emr-marketing-partner-sa:$LF_MARKETING_PARTNER_ROLE:ë§ˆì¼€íŒ… íƒ€ê²Ÿ ë¶„ì„ (ê°•ë‚¨êµ¬ 20-30ëŒ€)"
)

# ê²°ê³¼ ì €ì¥ìš© S3 ë²„í‚·
RESULTS_BUCKET="seoul-bike-analytics-results-${ACCOUNT_ID}"
aws s3 mb s3://$RESULTS_BUCKET --region $REGION 2>/dev/null || echo "ê²°ê³¼ ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤."

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p job-templates pod-templates

echo ""
echo "â„¹ï¸  Lake Formation FGAC + Apache Iceberg (Data Cells Filter ë°©ì‹)ê°€ í™œì„±í™”ëœ Virtual Clusterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
echo "â„¹ï¸  Spark Catalog: glue_catalog"
echo "â„¹ï¸  FGAC ë°©ì‹: Data Cells Filter (Hybrid Access Mode ë¶ˆí•„ìš”)"
echo "â„¹ï¸  Security Configuration: $SECURITY_CONFIG_ID"
echo "â„¹ï¸  Session Tag: LakeFormationAuthorizedCaller=EMRonEKSEngine"

# Job Template ìƒì„± í•¨ìˆ˜ (ëŒ€í™” ê¸°ë¡ ê¸°ë°˜ ìˆ˜ì •)
create_job_template() {
    local job_name=$1
    local service_account=$2
    local role_name=$3
    local timestamp=$(date +%Y%m%d-%H%M%S)
    
    echo "ğŸ“ $job_name Job Template ìƒì„± ì¤‘..."
    
    # Job Template íŒŒì¼ ìƒì„±
    local job_template_file="job-templates/${job_name}-job-template.json"
    
    cat > "$job_template_file" << EOF
{
  "name": "seoul-bike-${job_name}-${timestamp}",
  "virtualClusterId": "$LF_VIRTUAL_CLUSTER_ID",
  "executionRoleArn": "arn:aws:iam::${ACCOUNT_ID}:role/${role_name}",
  "releaseLabel": "emr-7.8.0-latest",
  "jobDriver": {
    "sparkSubmitJobDriver": {
      "entryPoint": "s3://${SCRIPTS_BUCKET}/spark-jobs/${job_name}-analysis.py",
      "sparkSubmitParameters": "--conf spark.executor.instances=2 --conf spark.executor.memory=1g --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.driver.memory=1g"
    }
  },
  "configurationOverrides": {
    "applicationConfiguration": [
      {
        "classification": "spark-defaults",
        "properties": {
          "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,com.amazonaws.emr.recordserver.connector.spark.sql.RecordServerSQLExtension",
          "spark.sql.catalog.glue_catalog": "org.apache.iceberg.spark.SparkCatalog",
          "spark.sql.catalog.glue_catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
          "spark.sql.catalog.glue_catalog.warehouse": "s3://${ICEBERG_BUCKET_NAME}/",
          "spark.sql.catalog.glue_catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
          "spark.sql.defaultCatalog": "glue_catalog",
          "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider",
          "spark.hadoop.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider",
          "spark.hadoop.fs.s3a.endpoint.region": "$REGION",
          "spark.hadoop.fs.s3.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
          "spark.hadoop.aws.region": "$REGION",
          "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
          "spark.sql.catalog.glue_catalog.client.region": "$REGION",
          "spark.sql.catalog.glue_catalog.s3.region": "$REGION",
          "spark.sql.catalog.glue_catalog.glue.region": "$REGION",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.client.region": "$REGION",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.s3.region": "$REGION",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.glue.region": "$REGION",
          "spark.hadoop.hive.metastore.glue.region": "$REGION",
          "spark.sql.catalog.glue_catalog.glue.lakeformation-enabled": "true",
          "spark.sql.secureCatalog": "glue_catalog",
          "spark.sql.catalog.glue_catalog.glue.account-id": "$ACCOUNT_ID",
          "spark.hadoop.iceberg.mr.catalog": "glue_catalog",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.warehouse": "s3://${ICEBERG_BUCKET_NAME}/",
          "spark.hadoop.iceberg.mr.catalog.glue_catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
          "spark.dynamicAllocation.maxExecutors": "4",
          "spark.dynamicAllocation.minExecutors": "0",
          "spark.dynamicAllocation.preallocateExecutors": "false",
          "spark.executor.instances": "2",
          "spark.executor.memory": "1g",
          "spark.executor.cores": "1",
          "spark.driver.memory": "1g",
          "spark.driver.cores": "1"
        }
      }
    ],
    "monitoringConfiguration": {
      "persistentAppUI": "ENABLED",
      "cloudWatchMonitoringConfiguration": {
        "logGroupName": "/aws/emr-containers/jobs",
        "logStreamNamePrefix": "${job_name}"
      },
      "s3MonitoringConfiguration": {
        "logUri": "s3://${RESULTS_BUCKET}/logs/"
      }
    }
  },
  "tags": {
    "LakeFormationAuthorizedCaller": "EMRonEKSEngine",
    "JobType": "LakeFormationFGAC",
    "Role": "${role_name}",
    "Namespace": "$USER_NAMESPACE",
    "CatalogType": "GlueCatalog",
    "FGACMethod": "DataCellsFilter"
  }
}
EOF
    
    echo "   âœ… Job Template ìƒì„± ì™„ë£Œ: $job_template_file"
    return 0
}

# Pod Template ìƒì„± í•¨ìˆ˜
create_pod_template() {
    local job_name=$1
    local service_account=$2
    
    echo "ğŸ“ $job_name Pod Template ìƒì„± ì¤‘..."
    
    # Pod Template íŒŒì¼ ìƒì„±
    local pod_template_file="pod-templates/${job_name}-pod-template.yaml"
    
    cat > "$pod_template_file" << EOF
apiVersion: v1
kind: Pod
metadata:
  name: spark-${job_name}-template
  namespace: $USER_NAMESPACE
  labels:
    app: spark-${job_name}
    version: "1.0"
    component: spark-executor
    spark-role: executor
    job-type: lake-formation-fgac
    LakeFormationAuthorizedCaller: "$LF_SESSION_TAG_VALUE"
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics/executors/prometheus/"
    prometheus.io/port: "4040"
    LakeFormationAuthorizedCaller: "$LF_SESSION_TAG_VALUE"
spec:
  serviceAccountName: ${service_account}
  restartPolicy: Never
  nodeSelector:
    karpenter.sh/nodepool: spark-compute-optimized
    node.kubernetes.io/instance-type: "c5.large"
  tolerations:
    - key: spark-compute-optimized
      operator: Equal
      value: "true"
      effect: NoSchedule
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: karpenter.sh/nodepool
            operator: In
            values:
            - spark-compute-optimized
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: spark-role
              operator: In
              values:
              - executor
          topologyKey: kubernetes.io/hostname
  containers:
  - name: spark-kubernetes-executor
    image: public.ecr.aws/emr-on-eks/spark/emr-7.8.0:latest
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
        ephemeral-storage: "2Gi"
      limits:
        memory: "1Gi"
        cpu: "1"
        ephemeral-storage: "4Gi"
    env:
    - name: SPARK_CONF_DIR
      value: /opt/spark/conf
    - name: AWS_REGION
      value: ${REGION}
    - name: AWS_DEFAULT_REGION
      value: ${REGION}
    - name: SPARK_LOCAL_DIRS
      value: /tmp/spark-local
    - name: JOB_NAME
      value: ${job_name}
    - name: SERVICE_ACCOUNT
      value: ${service_account}
    - name: LAKE_FORMATION_AUTHORIZED_CALLER
      value: "$LF_SESSION_TAG_VALUE"
    volumeMounts:
    - name: spark-local-dir
      mountPath: /tmp/spark-local
    - name: spark-conf-volume
      mountPath: /opt/spark/conf
    securityContext:
      runAsUser: 999
      runAsGroup: 1000
      fsGroup: 1000
      runAsNonRoot: true
  volumes:
  - name: spark-local-dir
    emptyDir:
      sizeLimit: 2Gi
  - name: spark-conf-volume
    emptyDir: {}
  terminationGracePeriodSeconds: 30
  dnsPolicy: ClusterFirst
  schedulerName: default-scheduler
EOF
    
    echo "   âœ… Pod Template ìƒì„± ì™„ë£Œ: $pod_template_file"
    return 0
}

# Job ì‹¤í–‰ í•¨ìˆ˜ (Template ì‚¬ìš©)
run_emr_job_with_template() {
    local job_name=$1
    local service_account=$2
    local role_name=$3
    local description=$4
    
    echo ""
    echo "ğŸš€ $job_name Job ì‹¤í–‰ ì¤‘ (Blueprint Template ì‚¬ìš©)..."
    echo "   ì„¤ëª…: $description"
    echo "   ì„œë¹„ìŠ¤ ê³„ì •: $service_account"
    echo "   IAM ì—­í• : $role_name"
    
    # Job Templateê³¼ Pod Template ìƒì„±
    create_job_template "$job_name" "$service_account" "$role_name"
    create_pod_template "$job_name" "$service_account"
    

    
    # Job Template íŒŒì¼ ì½ê¸°
    local job_template_file="job-templates/${job_name}-job-template.json"
    
    if [ ! -f "$job_template_file" ]; then
        echo "   âŒ Job Template íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $job_template_file"
        return 1
    fi
    
    # Job ì‹¤í–‰ (Template íŒŒì¼ ì‚¬ìš©)
    echo "   ğŸ“‹ Job Template íŒŒì¼ ì‚¬ìš©: $job_template_file"
    
    JOB_ID=$(aws emr-containers start-job-run \
        --region $REGION \
        --cli-input-json file://"$job_template_file" \
        --query 'id' \
        --output text)
    
    if [ -n "$JOB_ID" ] && [ "$JOB_ID" != "None" ]; then
        echo "   âœ… Job ì‹œì‘ë¨: $JOB_ID"
        echo "   ğŸ“Š ëª¨ë‹ˆí„°ë§: aws emr-containers describe-job-run --region $REGION --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID --id $JOB_ID"
        echo "   ğŸ“ Job Template: $job_template_file"
        echo "   ğŸ“ Pod Template: pod-templates/${job_name}-pod-template.yaml"
        echo "   ğŸ” Lake Formation FGAC: í™œì„±í™”ë¨ (Session Tag: $LF_SESSION_TAG_VALUE)"
        
        # Job IDë¥¼ íŒŒì¼ì— ì €ì¥
        echo "$job_name:$JOB_ID:$job_template_file" >> /tmp/emr-job-ids.txt
        
        return 0
    else
        echo "   âŒ Job ì‹œì‘ ì‹¤íŒ¨"
        return 1
    fi
}

# ëª¨ë“  Job ì‹¤í–‰
echo "1. EMR Job Template ë° Pod Template ìƒì„± ë° ì‹¤í–‰..."
rm -f /tmp/emr-job-ids.txt

for job_config in "${JOB_CONFIGS[@]}"; do
    IFS=':' read -r job_name service_account role_name description <<< "$job_config"
    
    if run_emr_job_with_template "$job_name" "$service_account" "$role_name" "$description"; then
        echo "   Job ì‹¤í–‰ ì„±ê³µ: $job_name"
    else
        echo "   Job ì‹¤í–‰ ì‹¤íŒ¨: $job_name"
    fi
    
    # Job ê°„ ê°„ê²©
    sleep 5
done

# Job ìƒíƒœ ëª¨ë‹ˆí„°ë§
echo ""
echo "2. Job ìƒíƒœ ëª¨ë‹ˆí„°ë§..."

if [ -f "/tmp/emr-job-ids.txt" ]; then
    echo ""
    echo "ğŸ“Š ì‹¤í–‰ëœ Job ëª©ë¡ (Blueprint Template ì‚¬ìš©):"
    echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
    echo "â”‚ Job ì´ë¦„                â”‚ Job ID                              â”‚ Template íŒŒì¼                       â”‚"
    echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
    
    while IFS=':' read -r job_name job_id template_file; do
        printf "â”‚ %-23s â”‚ %-35s â”‚ %-35s â”‚\n" "$job_name" "$job_id" "$(basename "$template_file")"
    done < /tmp/emr-job-ids.txt
    
    echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
    
    echo ""
    echo "3. Job ìƒíƒœ í™•ì¸ ì¤‘..."
    
    # ê° Jobì˜ ìƒíƒœ í™•ì¸
    while IFS=':' read -r job_name job_id template_file; do
        echo ""
        echo "   $job_name Job ìƒíƒœ í™•ì¸ ì¤‘..."
        
        # ìµœëŒ€ 2.5ë¶„ ëŒ€ê¸° (5íšŒ retry)
        for i in {1..5}; do
            JOB_STATE=$(aws emr-containers describe-job-run \
                --region $REGION \
                --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID \
                --id $job_id \
                --query 'jobRun.state' \
                --output text 2>/dev/null || echo "UNKNOWN")
            
            case $JOB_STATE in
                "COMPLETED")
                    echo "   âœ… $job_name: ì™„ë£Œ"
                    break
                    ;;
                "FAILED"|"CANCELLED")
                    echo "   âŒ $job_name: ì‹¤íŒ¨ ($JOB_STATE)"
                    
                    # ì‹¤íŒ¨ ì›ì¸ ì¡°íšŒ
                    FAILURE_REASON=$(aws emr-containers describe-job-run \
                        --region $REGION \
                        --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID \
                        --id $job_id \
                        --query 'jobRun.failureReason' \
                        --output text 2>/dev/null || echo "Unknown")
                    
                    echo "      ì‹¤íŒ¨ ì›ì¸: $FAILURE_REASON"
                    break
                    ;;
                "RUNNING"|"PENDING"|"SUBMITTED")
                    echo "   â³ $job_name: ì§„í–‰ ì¤‘ ($JOB_STATE) - ${i}/5"
                    if [ $i -lt 5 ]; then
                        sleep 30
                    fi
                    ;;
                *)
                    echo "   â“ $job_name: ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ ($JOB_STATE)"
                    break
                    ;;
            esac
        done
        
        # ìµœì¢… ìƒíƒœê°€ RUNNINGì´ë©´ íƒ€ì„ì•„ì›ƒ ë©”ì‹œì§€
        if [ "$JOB_STATE" = "RUNNING" ] || [ "$JOB_STATE" = "PENDING" ]; then
            echo "   â° $job_name: íƒ€ì„ì•„ì›ƒ (ì—¬ì „íˆ ì‹¤í–‰ ì¤‘)"
        fi
        
    done < /tmp/emr-job-ids.txt
    
    echo ""
    echo "4. Job ê²°ê³¼ ë° Template ì •ë³´..."
    
    # Template íŒŒì¼ ìœ„ì¹˜ ì•ˆë‚´
    echo ""
    echo "ğŸ“ ìƒì„±ëœ Template íŒŒì¼:"
    echo "   Job Templates: ./job-templates/"
    echo "   Pod Templates: ./pod-templates/"
    ls -la job-templates/ | grep -E "\.json$" | awk '{print "     - " $9}'
    ls -la pod-templates/ | grep -E "\.yaml$" | awk '{print "     - " $9}'
    
    # ë¡œê·¸ ìœ„ì¹˜ ì•ˆë‚´
    echo ""
    echo "ğŸ“ Job ë¡œê·¸ ìœ„ì¹˜:"
    echo "   S3 ë²„í‚·: s3://$RESULTS_BUCKET/logs/"
    echo "   ë¡œì»¬ í™•ì¸: aws s3 ls s3://$RESULTS_BUCKET/logs/ --recursive"
    
    # Job ìƒì„¸ ì •ë³´ ì¡°íšŒ ëª…ë ¹ì–´ ì•ˆë‚´
    echo ""
    echo "ğŸ” Job ìƒì„¸ ì •ë³´ ì¡°íšŒ ëª…ë ¹ì–´:"
    while IFS=':' read -r job_name job_id template_file; do
        echo "   $job_name: aws emr-containers describe-job-run --region $REGION --virtual-cluster-id $LF_VIRTUAL_CLUSTER_ID --id $job_id"
    done < /tmp/emr-job-ids.txt
    
else
    echo "âŒ ì‹¤í–‰ëœ Jobì´ ì—†ìŠµë‹ˆë‹¤."
fi

# ì„ì‹œ íŒŒì¼ ì •ë¦¬
rm -f /tmp/emr-job-ids.txt

echo ""
echo "=== EMR on EKS Job ì‹¤í–‰ ì™„ë£Œ (Lake Formation FGAC + Apache Iceberg) ==="
echo ""
echo "ğŸ“‹ ì‹¤í–‰ ìš”ì•½:"
echo "   â€¢ ì´ 4ê°œ ì—­í• ë³„ ë¶„ì„ Job ì‹¤í–‰"
echo "   â€¢ Lake Formation FGAC Virtual Cluster ì‚¬ìš©: $LF_VIRTUAL_CLUSTER_ID"
echo "   â€¢ Security Configuration ì ìš©: $SECURITY_CONFIG_ID"
echo "   â€¢ Session Tag ì„¤ì •: LakeFormationAuthorizedCaller=EMRonEKSEngine"
echo "   â€¢ Spark Catalog: glue_catalog"
echo "   â€¢ S3 + Apache Iceberg í™˜ê²½"
echo "   â€¢ FGAC ë°©ì‹: Data Cells Filter"
echo "   â€¢ Lake Formation ê¶Œí•œ: Data Cells Filter ê¸°ë°˜"
echo ""
echo "ğŸ—‚ï¸ Apache Iceberg ì¹´íƒˆë¡œê·¸ ì„¤ì •:"
echo "   â€¢ ì¹´íƒˆë¡œê·¸ëª…: glue_catalog (AWS ê³µì‹ ë¬¸ì„œ ê¸°ì¤€)"
echo "   â€¢ ê¸°ë³¸ ì¹´íƒˆë¡œê·¸: spark.sql.defaultCatalog=glue_catalog"
echo "   â€¢ í…Œì´ë¸” ì°¸ì¡°: glue_catalog.bike_db.bike_rental_data"
echo "   â€¢ Warehouse: s3://${ICEBERG_BUCKET_NAME}/"
echo "   â€¢ Catalog Implementation: org.apache.iceberg.aws.glue.GlueCatalog"
echo ""
echo "ğŸ” Lake Formation FGAC ì ìš© ê²°ê³¼ (ëŒ€í™” ê¸°ë¡ ê¸°ë°˜):"
echo "   â€¢ DataSteward: 100,000ê±´ ì „ì²´ ë¶„ì„ (ëª¨ë“  ì»¬ëŸ¼ ì ‘ê·¼)"
echo "   â€¢ GangnamAnalytics: ~3,000ê±´ (ê°•ë‚¨êµ¬ë§Œ, birth_year ì œì™¸)"
echo "   â€¢ Operation: 100,000ê±´ (ê°œì¸ì •ë³´ ì œì™¸: birth_year, gender)"
echo "   â€¢ MarketingPartner: ~2,000ê±´ (ê°•ë‚¨êµ¬ 20-30ëŒ€ë§Œ, birth_year ì œì™¸)"
echo ""
echo "ğŸ“ Template ì¬ì‚¬ìš©:"
echo "   â€¢ Job Templates: ./job-templates/ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥"
echo "   â€¢ Pod Templates: ./pod-templates/ ë””ë ‰í† ë¦¬ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥"
echo "   â€¢ í–¥í›„ ìœ ì‚¬í•œ Job ì‹¤í–‰ ì‹œ Template ìˆ˜ì •í•˜ì—¬ í™œìš©"
echo ""
echo "ğŸ¯ Lake Formation FGAC + Apache Iceberg ê²€ì¦ (Data Cells Filter ë°©ì‹):"
echo "   â€¢ âœ… Data Cells Filter ì„¤ì •: ì—­í• ë³„ í–‰/ì»¬ëŸ¼ í•„í„°ë§ ì ìš©"
echo "   â€¢ âœ… Lake Formation FGAC: Multi-dimensional ì ‘ê·¼ ì œì–´"
echo "   â€¢ âœ… Spark Catalog ì„¤ì •: glue_catalog ì‚¬ìš© (AWS ê³µì‹ ê¶Œì¥)"
echo "   â€¢ âœ… Session Tag ì„¤ì •: EMRonEKSEngine (ì˜¬ë°”ë¥¸ ëŒ€ì†Œë¬¸ì)"
echo "   â€¢ âœ… EMR on EKS FGAC êµ¬ì„±: Security Configuration, QueryEngine Role ì •ìƒ"
echo ""
echo "ğŸ”§ ê°œì„  ì‚¬í•­ (EMR Serverless ë°©ì‹ ì°¸ì¡°):"
echo "   â€¢ ê°œì„ ì : Hybrid Access Mode ìš”êµ¬ì‚¬í•­ ì œê±°"
echo "   â€¢ í•´ê²°ì±…: Data Cells Filter ë°©ì‹ìœ¼ë¡œ ì™„ì „í•œ FGAC êµ¬í˜„"
echo "   â€¢ ê²€ì¦: EMR Serverlessì—ì„œ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì„±ê³µ í™•ì¸"
echo ""
echo "ğŸ“š í•™ìŠµ ì‚¬í•­:"
echo "   â€¢ Data Cells Filter ë°©ì‹ì´ Hybrid Access Modeë³´ë‹¤ ì•ˆì •ì "
echo "   â€¢ EMR on EKS FGACëŠ” Data Cells Filter ê¸°ë°˜ ê¶Œí•œ ëª¨ë¸ ì‚¬ìš©"
echo "   â€¢ spark.sql.catalog.glue_catalog ë„¤ì´ë°ì´ AWS í‘œì¤€"
echo "   â€¢ EMR Serverlessì™€ EMR on EKS ëª¨ë‘ ë™ì¼í•œ FGAC ë°©ì‹ ì§€ì›"
echo ""
echo "âœ… ë‹¤ìŒ ë‹¨ê³„: ./scripts/08-verify-and-analyze.sh"
