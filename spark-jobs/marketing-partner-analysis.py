#!/usr/bin/env python3
"""
Marketing Partner Analysis - ë§ˆì¼€íŒ… íŒŒíŠ¸ë„ˆ ë¶„ì„
Lake Formation FGAC ë°ëª¨ìš© - ê°•ë‚¨êµ¬ 20-30ëŒ€ ë°ì´í„°ë§Œ ì ‘ê·¼ (ê°œì¸ì •ë³´ birth_year ì œì™¸)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    """Lake Formation FGAC ì§€ì› Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName("MarketingPartner-TargetAnalysis") \
        .getOrCreate()

def main():
    print("=== Marketing Partner Analysis ì‹œì‘ ===")
    print("ì—­í• : ë§ˆì¼€íŒ… íŒŒíŠ¸ë„ˆ - íƒ€ê²Ÿ ê³ ê° ë¶„ì„ (ê°œì¸ì •ë³´ ì œì™¸)")
    
    spark = create_spark_session()
    
    try:
        # Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ì½ê¸°
        print("\n1. Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ ì¤‘...")
        namespace = "bike_db"
        table_name = "bike_rental_data"
        
        print(f"   ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
        print(f"   í…Œì´ë¸”ëª…: {table_name}")
        print(f"   ì¹´íƒˆë¡œê·¸: glue_catalog.{namespace}.{table_name}")
        
        # Glue ì¹´íƒˆë¡œê·¸ë¥¼ í†µí•´ ë°ì´í„° ì½ê¸°
        df = spark.read.table(f"glue_catalog.{namespace}.{table_name}")
        
        total_records = df.count()
        print(f"âœ… Lake Formation í•„í„°ë§ í›„ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê±´")
        print("ğŸ”’ ì ‘ê·¼ ì œí•œ: ê°œì¸ì •ë³´ birth_year ì œì™¸")
        
        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¸
        print("\n2. ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„° ìŠ¤í‚¤ë§ˆ:")
        df.printSchema()
        
        # ê¸°ë³¸ í†µê³„
        print(f"\n3. ê¸°ë³¸ í†µê³„ ì •ë³´:")
        print(f"   â€¢ ê³ ìœ  ëŒ€ì—¬ ID: {df.select('rental_id').distinct().count():,}")
        print(f"   â€¢ ê³ ìœ  ì •ê±°ì¥: {df.select('station_id').distinct().count():,}")
        print(f"   â€¢ ê³ ìœ  êµ¬: {df.select('district').distinct().count()}")
        
        # êµ¬ë³„ ë¶„í¬ í™•ì¸ (Lake Formation í•„í„°ë§ ê²°ê³¼)
        print(f"\n4. êµ¬ë³„ ë¶„í¬ (Lake Formation í•„í„°ë§ ê²°ê³¼):")
        district_stats = df.groupBy("district") \
                           .agg(count("*").alias("count"),
                                avg("usage_min").alias("avg_usage"),
                                avg("distance_meter").alias("avg_distance")) \
                           .orderBy(desc("count"))
        
        district_stats.show(truncate=False)
        
        # ì„±ë³„ ë¶„í¬ (íƒ€ê²Ÿ ë§ˆì¼€íŒ…ìš©)
        print(f"\n5. ì„±ë³„ ë¶„í¬ (íƒ€ê²Ÿ ë§ˆì¼€íŒ…ìš©):")
        gender_stats = df.groupBy("gender") \
                         .agg(count("*").alias("count"),
                              avg("usage_min").alias("avg_usage"),
                              avg("distance_meter").alias("avg_distance")) \
                         .orderBy(desc("count"))
        
        gender_stats.show()
        
        # ì¸ê¸° ì •ê±°ì¥ ë¶„ì„ (ë§ˆì¼€íŒ… í¬ì¸íŠ¸)
        print(f"\n6. ì¸ê¸° ì •ê±°ì¥ ë¶„ì„ (ë§ˆì¼€íŒ… í¬ì¸íŠ¸ - ìƒìœ„ 10ê°œ):")
        popular_stations = df.groupBy("station_id", "station_name") \
                             .agg(count("*").alias("rental_count"),
                                  avg("usage_min").alias("avg_usage"),
                                  countDistinct("rental_id").alias("unique_rentals")) \
                             .orderBy(desc("rental_count")) \
                             .limit(10)
        
        popular_stations.show(truncate=False)
        
        # ëŒ€ì—¬ ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ë§ˆì¼€íŒ… íƒ€ì´ë°)
        print(f"\n7. ëŒ€ì—¬ ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ë§ˆì¼€íŒ… íƒ€ì´ë°):")
        usage_patterns = df.select(
            avg("usage_min").alias("í‰ê· _ëŒ€ì—¬ì‹œê°„"),
            min("usage_min").alias("ìµœì†Œ_ëŒ€ì—¬ì‹œê°„"),
            max("usage_min").alias("ìµœëŒ€_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.5)").alias("ì¤‘ì•™ê°’_ëŒ€ì—¬ì‹œê°„")
        )
        
        usage_patterns.show()
        
        # ì´ë™ ê±°ë¦¬ íŒ¨í„´ (ì„œë¹„ìŠ¤ ë²”ìœ„ ë¶„ì„)
        print(f"\n8. ì´ë™ ê±°ë¦¬ íŒ¨í„´ (ì„œë¹„ìŠ¤ ë²”ìœ„ ë¶„ì„):")
        distance_patterns = df.select(
            avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬"),
            min("distance_meter").alias("ìµœì†Œ_ì´ë™ê±°ë¦¬"),
            max("distance_meter").alias("ìµœëŒ€_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.5)").alias("ì¤‘ì•™ê°’_ì´ë™ê±°ë¦¬")
        )
        
        distance_patterns.show()
        
        # ì‹œê°„ëŒ€ë³„ ì´ìš© íŒ¨í„´ (ë§ˆì¼€íŒ… ì‹œê°„ëŒ€ ë¶„ì„)
        print(f"\n9. ì‹œê°„ëŒ€ë³„ ì´ìš© íŒ¨í„´ (ë§ˆì¼€íŒ… ì‹œê°„ëŒ€ ë¶„ì„):")
        hourly_marketing = df.withColumn("hour", hour("rental_date")) \
                             .groupBy("hour") \
                             .agg(count("*").alias("rental_count"),
                                  avg("usage_min").alias("avg_usage")) \
                             .orderBy("hour")
        
        hourly_marketing.show(24)
        
        # ì‚¬ìš©ì ìœ í˜•ë³„ ë§ˆì¼€íŒ… ë¶„ì„
        print(f"\n10. ì‚¬ìš©ì ìœ í˜•ë³„ ë§ˆì¼€íŒ… ë¶„ì„:")
        user_marketing = df.groupBy("user_type") \
                           .agg(count("*").alias("count"),
                                avg("usage_min").alias("avg_usage"),
                                avg("distance_meter").alias("avg_distance"),
                                countDistinct("station_id").alias("station_variety")) \
                           .orderBy(desc("count"))
        
        user_marketing.show()
        
        # ìš”ì¼ë³„ ì´ìš© íŒ¨í„´ (ë§ˆì¼€íŒ… ìº í˜ì¸ íƒ€ì´ë°)
        print(f"\n11. ìš”ì¼ë³„ ì´ìš© íŒ¨í„´ (ë§ˆì¼€íŒ… ìº í˜ì¸ íƒ€ì´ë°):")
        weekday_pattern = df.withColumn("weekday", date_format("rental_date", "EEEE")) \
                            .groupBy("weekday") \
                            .agg(count("*").alias("rental_count"),
                                 avg("usage_min").alias("avg_usage")) \
                            .orderBy(desc("rental_count"))
        
        weekday_pattern.show()
        
        # ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìš”ì•½
        print(f"\n12. ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ìš”ì•½:")
        
        # ìµœê³  ì´ìš© ì‹œê°„ëŒ€
        peak_hour = df.withColumn("hour", hour("rental_date")) \
                      .groupBy("hour") \
                      .count() \
                      .orderBy(desc("count")) \
                      .first()
        
        print(f"   â€¢ ìµœê³  ì´ìš© ì‹œê°„ëŒ€: {peak_hour['hour']}ì‹œ ({peak_hour['count']:,}ê±´)")
        
        # í‰ê·  ì´ìš© ì‹œê°„
        avg_usage = df.agg(avg("usage_min")).collect()[0][0]
        print(f"   â€¢ í‰ê·  ì´ìš© ì‹œê°„: {avg_usage:.1f}ë¶„")
        
        # í‰ê·  ì´ë™ ê±°ë¦¬
        avg_distance = df.agg(avg("distance_meter")).collect()[0][0]
        print(f"   â€¢ í‰ê·  ì´ë™ ê±°ë¦¬: {avg_distance:.0f}m")
        
        print(f"\n=== Marketing Partner Analysis ì™„ë£Œ ===")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_records:,}ê±´")
        print(f"ğŸ”’ ê¶Œí•œ: Lake Formation FGAC ì ìš© (ê°œì¸ì •ë³´ birth_year ì œì™¸)")
        print(f"ğŸ“Š ì—­í• : íƒ€ê²Ÿ ë§ˆì¼€íŒ… ë¶„ì„ ë° ìº í˜ì¸ ê¸°íš")
        print(f"ğŸ—‚ï¸ ì¹´íƒˆë¡œê·¸: Glue Catalog (Apache Iceberg)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
