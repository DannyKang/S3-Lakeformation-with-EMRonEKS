#!/bin/bash

# EMR Serverless Lake Formation FGAC ì‹¤ì œ ê¶Œí•œ í…ŒìŠ¤íŠ¸
# ê° ì—­í• ë³„ë¡œ ./spark-jobs/ ë””ë ‰í† ë¦¬ì˜ ì‹¤ì œ ë¶„ì„ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ FGAC ê¸°ëŠ¥ ê²€ì¦

set -e

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
if [ ! -f ".env" ]; then
    echo "âŒ .env íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    exit 1
fi

source .env

echo "=== EMR Serverless Lake Formation FGAC ì‹¤ì œ ê¶Œí•œ í…ŒìŠ¤íŠ¸ ==="
echo "ëª©ì : ê° ì—­í• ë³„ ì°¨ë³„í™”ëœ ë°ì´í„° ì ‘ê·¼ ê¶Œí•œ ì‹¤ì œ ê²€ì¦"
echo "ë°ì´í„°: ì„œìš¸ì‹œ ë”°ë¦‰ì´ ìì „ê±° ëŒ€ì—¬ ë°ì´í„° 100,000ê±´"
echo "ë¶„ì„ ì½”ë“œ: ./spark-jobs/ ë””ë ‰í† ë¦¬ì˜ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì½”ë“œ í™œìš©"
echo ""

# EMR Serverless ì„¤ì •
EMR_APP_NAME="seoul-bike-fgac-test"
EMR_RELEASE_LABEL="emr-7.8.0"

# ê¸°ì¡´ ì• í”Œë¦¬ì¼€ì´ì…˜ í™•ì¸
EXISTING_APP=$(aws emr-serverless list-applications \
    --region $REGION \
    --query "applications[?name=='$EMR_APP_NAME'].id" \
    --output text)

if [ -n "$EXISTING_APP" ] && [ "$EXISTING_APP" != "None" ]; then
    echo "ê¸°ì¡´ EMR Serverless ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ìš©: $EXISTING_APP"
    EMR_APP_ID=$EXISTING_APP
else
    echo "ìƒˆ EMR Serverless ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ì¤‘..."
    
    EMR_APP_ID=$(aws emr-serverless create-application \
        --release-label $EMR_RELEASE_LABEL \
        --type "SPARK" \
        --name $EMR_APP_NAME \
        --runtime-configuration '{
            "classification": "spark-defaults",
            "properties": {
                "spark.emr-serverless.lakeformation.enabled": "true",
                "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
                "spark.sql.catalog.glue_catalog": "org.apache.iceberg.spark.SparkCatalog",
                "spark.sql.catalog.glue_catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
                "spark.sql.catalog.glue_catalog.warehouse": "s3://'$ICEBERG_BUCKET_NAME'/",
                "spark.sql.catalog.glue_catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
                "spark.sql.defaultCatalog": "glue_catalog",
                "spark.sql.catalog.glue_catalog.glue.lakeformation-enabled": "true",
                "spark.sql.catalog.glue_catalog.client.region": "'$REGION'",
                "spark.sql.catalog.glue_catalog.s3.region": "'$REGION'",
                "spark.sql.catalog.glue_catalog.glue.region": "'$REGION'",
                "spark.sql.catalog.glue_catalog.glue.account-id": "'$ACCOUNT_ID'"
            }
        }' \
        --auto-start-configuration '{"enabled": true}' \
        --auto-stop-configuration '{"enabled": true, "idleTimeoutMinutes": 30}' \
        --maximum-capacity '{
            "cpu": "8 vCPU",
            "memory": "32 GB"
        }' \
        --region $REGION \
        --query 'applicationId' \
        --output text)
    
    echo "âœ… ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ì™„ë£Œ: $EMR_APP_ID"
fi

# ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ í™•ì¸
echo "ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ í™•ì¸ ì¤‘..."
for i in {1..15}; do
    APP_STATE=$(aws emr-serverless get-application \
        --application-id $EMR_APP_ID \
        --region $REGION \
        --query 'application.state' \
        --output text)
    
    echo "   ìƒíƒœ ${i}/15: $APP_STATE"
    
    if [ "$APP_STATE" = "STARTED" ]; then
        echo "âœ… ì• í”Œë¦¬ì¼€ì´ì…˜ ì¤€ë¹„ ì™„ë£Œ"
        break
    elif [ "$APP_STATE" = "CREATED" ] || [ "$APP_STATE" = "STOPPED" ]; then
        echo "   ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì¤‘..."
        aws emr-serverless start-application \
            --application-id $EMR_APP_ID \
            --region $REGION 2>/dev/null || true
    fi
    
    sleep 30
done

if [ "$APP_STATE" != "STARTED" ]; then
    echo "âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹¤íŒ¨"
    exit 1
fi

echo ""
echo "ğŸ“‹ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:"
echo "   1. DataSteward: ì „ì²´ ë°ì´í„° + ì „ì²´ ì»¬ëŸ¼ (ê°œì¸ì •ë³´ í¬í•¨) - data-steward-analysis.py"
echo "   2. GangnamAnalytics: ê°•ë‚¨êµ¬ ë°ì´í„°ë§Œ + birth_year ì œì™¸ - gangnam-analytics-analysis.py"
echo "   3. Operation: ì „ì²´ ë°ì´í„° + ìš´ì˜ ì»¬ëŸ¼ë§Œ (ê°œì¸ì •ë³´ ì œì™¸) - operation-analysis.py"
echo "   4. MarketingPartner: ê°•ë‚¨êµ¬ 20-30ëŒ€ë§Œ + birth_year ì œì™¸ - marketing-partner-analysis.py"
echo ""

# Spark ë¶„ì„ ì½”ë“œë¥¼ S3ì— ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
upload_spark_jobs() {
    echo "ğŸ“ Spark ë¶„ì„ ì½”ë“œë¥¼ S3ì— ì—…ë¡œë“œ ì¤‘..."
    
    # spark-jobs ë””ë ‰í† ë¦¬ í™•ì¸
    if [ ! -d "./spark-jobs" ]; then
        echo "âŒ ./spark-jobs ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        exit 1
    fi
    
    # ê° ì—­í• ë³„ ë¶„ì„ ì½”ë“œ ì—…ë¡œë“œ
    local spark_jobs=(
        "data-steward-analysis.py"
        "gangnam-analytics-analysis.py"
        "operation-analysis.py"
        "marketing-partner-analysis.py"
    )
    
    for job_file in "${spark_jobs[@]}"; do
        if [ -f "./spark-jobs/$job_file" ]; then
            echo "   ğŸ“¤ ì—…ë¡œë“œ ì¤‘: $job_file"
            aws s3 cp "./spark-jobs/$job_file" "s3://$SCRIPTS_BUCKET/spark-jobs/$job_file" --region $REGION
            echo "      âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://$SCRIPTS_BUCKET/spark-jobs/$job_file"
        else
            echo "   âŒ íŒŒì¼ ì—†ìŒ: ./spark-jobs/$job_file"
        fi
    done
    
    echo "âœ… ëª¨ë“  Spark ë¶„ì„ ì½”ë“œ ì—…ë¡œë“œ ì™„ë£Œ"
    echo ""
}

# Spark ì½”ë“œ ì—…ë¡œë“œ ì‹¤í–‰
upload_spark_jobs
# ì—­í• ë³„ ìƒì„¸ í…ŒìŠ¤íŠ¸ ì •ì˜ (ì‹¤ì œ spark-jobs ë””ë ‰í† ë¦¬ ì½”ë“œ ì‚¬ìš©)
declare -a FGAC_TEST_ROLES=(
    "LF_DataStewardRole|ë°ì´í„° ê´€ë¦¬ì|ì „ì²´ ë°ì´í„° + ì „ì²´ ì»¬ëŸ¼ (ê°œì¸ì •ë³´ í¬í•¨)|data-steward-analysis.py"
    "LF_GangnamAnalyticsRole|ê°•ë‚¨êµ¬ ë¶„ì„ê°€|ê°•ë‚¨êµ¬ ë°ì´í„°ë§Œ + birth_year ì œì™¸|gangnam-analytics-analysis.py"
    "LF_OperationRole|ìš´ì˜íŒ€|ì „ì²´ ë°ì´í„° + ìš´ì˜ ì»¬ëŸ¼ë§Œ (ê°œì¸ì •ë³´ ì œì™¸)|operation-analysis.py"
    "LF_MarketingPartnerRole|ë§ˆì¼€íŒ… íŒŒíŠ¸ë„ˆ|ê°•ë‚¨êµ¬ 20-30ëŒ€ë§Œ + birth_year ì œì™¸|marketing-partner-analysis.py"
)

# Job ì‹¤í–‰ í•¨ìˆ˜
run_fgac_analysis_job() {
    local role_name=$1
    local role_description=$2
    local test_scenario=$3
    local spark_job_file=$4
    
    echo "ğŸš€ $role_name ($role_description) FGAC ë¶„ì„ Job ì‹¤í–‰ ì¤‘..."
    echo "   ì‹œë‚˜ë¦¬ì˜¤: $test_scenario"
    echo "   ë¶„ì„ ì½”ë“œ: $spark_job_file"
    
    JOB_RUN_ID=$(aws emr-serverless start-job-run \
        --application-id $EMR_APP_ID \
        --execution-role-arn "arn:aws:iam::$ACCOUNT_ID:role/$role_name" \
        --name "${role_name}-analysis-$(date +%Y%m%d-%H%M%S)" \
        --job-driver '{
            "sparkSubmit": {
                "entryPoint": "s3://'$SCRIPTS_BUCKET'/spark-jobs/'$spark_job_file'",
                "sparkSubmitParameters": "--conf spark.executor.cores=2 --conf spark.executor.memory=4g --conf spark.driver.cores=2 --conf spark.driver.memory=4g --conf spark.executor.instances=2"
            }
        }' \
        --configuration-overrides '{
            "monitoringConfiguration": {
                "s3MonitoringConfiguration": {
                    "logUri": "s3://'$SCRIPTS_BUCKET'/logs/fgac-analysis/"
                }
            }
        }' \
        --region $REGION \
        --query 'jobRunId' \
        --output text)
    
    if [ -n "$JOB_RUN_ID" ] && [ "$JOB_RUN_ID" != "None" ]; then
        echo "   âœ… Job ì‹œì‘ë¨: $JOB_RUN_ID"
        echo "$role_name|$JOB_RUN_ID|$role_description|$test_scenario|$spark_job_file" >> /tmp/fgac-analysis-job-ids.txt
        return 0
    else
        echo "   âŒ Job ì‹œì‘ ì‹¤íŒ¨"
        return 1
    fi
}

echo "ğŸ¯ EMR Serverless FGAC ì‹¤ì œ ë¶„ì„ Job ì‹¤í–‰ ì‹œì‘..."
echo ""

# ëª¨ë“  Job ì‹¤í–‰
rm -f /tmp/fgac-analysis-job-ids.txt

for role_config in "${FGAC_TEST_ROLES[@]}"; do
    IFS='|' read -r role_name role_description test_scenario spark_job_file <<< "$role_config"
    
    if run_fgac_analysis_job "$role_name" "$role_description" "$test_scenario" "$spark_job_file"; then
        echo "   âœ… $role_name Job ì‹¤í–‰ ì„±ê³µ"
    else
        echo "   âŒ $role_name Job ì‹¤í–‰ ì‹¤íŒ¨"
    fi
    
    echo ""
    sleep 10
done

# Job ìƒíƒœ ëª¨ë‹ˆí„°ë§
echo "ğŸ“Š ì‹¤í–‰ëœ FGAC ë¶„ì„ Job ëª©ë¡:"
echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
echo "â”‚ ì—­í•                     â”‚ Job Run ID                          â”‚ ì„¤ëª…                                â”‚ ë¶„ì„ ì½”ë“œ                           â”‚"
echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"

if [ -f "/tmp/fgac-analysis-job-ids.txt" ]; then
    while IFS='|' read -r role_name job_run_id role_description test_scenario spark_job_file; do
        printf "â”‚ %-23s â”‚ %-35s â”‚ %-35s â”‚ %-35s â”‚\n" "$role_name" "$job_run_id" "$role_description" "$spark_job_file"
    done < /tmp/fgac-analysis-job-ids.txt
fi

echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
echo ""

echo "â³ Job ì‹¤í–‰ ê²°ê³¼ í™•ì¸ (ìµœëŒ€ 15ë¶„ ëŒ€ê¸°)..."
echo ""

# ê° Jobì˜ ìƒíƒœ í™•ì¸
if [ -f "/tmp/fgac-analysis-job-ids.txt" ]; then
    while IFS='|' read -r role_name job_run_id role_description test_scenario spark_job_file; do
        echo "ğŸ“Š $role_name Job ìƒíƒœ í™•ì¸ ì¤‘..."
        echo "   ì‹œë‚˜ë¦¬ì˜¤: $test_scenario"
        echo "   ë¶„ì„ ì½”ë“œ: $spark_job_file"
        
        # ìµœëŒ€ 15ë¶„ ëŒ€ê¸° (30íšŒ retry)
        for i in {1..30}; do
            JOB_STATE=$(aws emr-serverless get-job-run \
                --application-id $EMR_APP_ID \
                --job-run-id $job_run_id \
                --region $REGION \
                --query 'jobRun.state' \
                --output text 2>/dev/null || echo "UNKNOWN")
            
            case $JOB_STATE in
                "SUCCESS")
                    echo "   âœ… ì™„ë£Œ - Lake Formation FGAC ë¶„ì„ ì„±ê³µ"
                    echo "   ğŸ“‹ ìƒì„¸ ë¡œê·¸: s3://$SCRIPTS_BUCKET/logs/fgac-analysis/applications/$EMR_APP_ID/jobs/$job_run_id/"
                    
                    # ì‹¤í–‰ ì‹œê°„ ì •ë³´ ì¶”ê°€
                    JOB_DURATION=$(aws emr-serverless get-job-run \
                        --application-id $EMR_APP_ID \
                        --job-run-id $job_run_id \
                        --region $REGION \
                        --query 'jobRun.totalExecutionDurationSeconds' \
                        --output text 2>/dev/null || echo "N/A")
                    
                    if [ "$JOB_DURATION" != "N/A" ] && [ "$JOB_DURATION" != "None" ]; then
                        echo "   â±ï¸  ì‹¤í–‰ ì‹œê°„: ${JOB_DURATION}ì´ˆ"
                    fi
                    break
                    ;;
                "FAILED"|"CANCELLED")
                    echo "   âŒ ì‹¤íŒ¨ ($JOB_STATE)"
                    
                    # ì‹¤íŒ¨ ì›ì¸ ì¡°íšŒ
                    FAILURE_REASON=$(aws emr-serverless get-job-run \
                        --application-id $EMR_APP_ID \
                        --job-run-id $job_run_id \
                        --region $REGION \
                        --query 'jobRun.stateDetails' \
                        --output text 2>/dev/null || echo "Unknown")
                    
                    echo "   ì‹¤íŒ¨ ì›ì¸: $FAILURE_REASON"
                    echo "   ğŸ“‹ ì˜¤ë¥˜ ë¡œê·¸: s3://$SCRIPTS_BUCKET/logs/fgac-analysis/applications/$EMR_APP_ID/jobs/$job_run_id/"
                    break
                    ;;
                "RUNNING"|"PENDING"|"SUBMITTED"|"SCHEDULED")
                    echo "   â³ ì§„í–‰ ì¤‘ ($JOB_STATE) - ${i}/30"
                    sleep 30
                    ;;
                *)
                    echo "   â“ ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ ($JOB_STATE) - ${i}/30"
                    sleep 30
                    ;;
            esac
        done
        
        if [ "$JOB_STATE" != "SUCCESS" ] && [ "$JOB_STATE" != "FAILED" ] && [ "$JOB_STATE" != "CANCELLED" ]; then
            echo "   â° ìƒíƒœ í™•ì¸ íƒ€ì„ì•„ì›ƒ (í˜„ì¬: $JOB_STATE)"
        fi
        
        echo ""
        
    done < /tmp/fgac-analysis-job-ids.txt
fi

echo "=" * 80
echo "ğŸ¯ EMR Serverless Lake Formation FGAC ì‹¤ì œ ë¶„ì„ ì™„ë£Œ"
echo "=" * 80
echo ""
echo "ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:"
echo "   â€¢ ê° ì—­í• ë³„ë¡œ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì½”ë“œë¥¼ í†µí•œ FGAC ê²€ì¦ ì™„ë£Œ"
echo "   â€¢ ./spark-jobs/ ë””ë ‰í† ë¦¬ì˜ ì‹¤ì œ ë¶„ì„ ì½”ë“œ í™œìš©"
echo "   â€¢ Lake Formation FGACì˜ ì‹¤ì œ ë™ì‘ ê²€ì¦"
echo "   â€¢ ì»¬ëŸ¼ ë ˆë²¨, í–‰ ë ˆë²¨ í•„í„°ë§ íš¨ê³¼ í™•ì¸"
echo ""
echo "ğŸ“ ìƒì„¸ ë¡œê·¸ ìœ„ì¹˜: s3://$SCRIPTS_BUCKET/logs/fgac-analysis/"
echo ""
echo "ğŸ” ë¡œê·¸ ë¶„ì„ ëª…ë ¹ì–´:"
echo "   aws s3 ls s3://$SCRIPTS_BUCKET/logs/fgac-analysis/applications/$EMR_APP_ID/jobs/ --region $REGION"
echo ""
echo "ğŸ“Š ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´:"
echo "   aws emr-serverless list-job-runs --application-id $EMR_APP_ID --region $REGION"
echo ""
echo "ğŸ‰ ë‹¤ìŒ ë‹¨ê³„:"
echo "   1. ê° ì—­í• ë³„ ë¡œê·¸ ë¶„ì„ìœ¼ë¡œ FGAC ë™ì‘ ìƒì„¸ í™•ì¸"
echo "   2. ê¶Œí•œ ì°¨ì´ì  ë¹„êµ ë¶„ì„"
echo "   3. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ê²°ê³¼ ê²€í† "
echo "   4. í•„ìš”ì‹œ ì¶”ê°€ Data Cells Filter ì¡°ì •"
echo ""
echo "ğŸ“‹ ë¶„ì„ ì½”ë“œë³„ ê¸°ëŒ€ ê²°ê³¼:"
echo "   â€¢ data-steward-analysis.py: ì „ì²´ 100,000ê±´ + ëª¨ë“  ì»¬ëŸ¼ ì ‘ê·¼"
echo "   â€¢ gangnam-analytics-analysis.py: ê°•ë‚¨êµ¬ ë°ì´í„°ë§Œ + birth_year ì œì™¸"
echo "   â€¢ operation-analysis.py: ì „ì²´ ë°ì´í„° + ìš´ì˜ ì»¬ëŸ¼ë§Œ"
echo "   â€¢ marketing-partner-analysis.py: ê°•ë‚¨êµ¬ 20-30ëŒ€ë§Œ + íƒ€ê²Ÿ ë¶„ì„"
