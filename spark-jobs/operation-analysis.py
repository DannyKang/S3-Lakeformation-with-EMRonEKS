#!/usr/bin/env python3
"""
Operation Analysis - ìš´ì˜íŒ€ ë¶„ì„
Lake Formation FGAC ë°ëª¨ìš© - ìš´ì˜ ê´€ë ¨ ë°ì´í„°ë§Œ ì ‘ê·¼ (ê°œì¸ì •ë³´ birth_year, gender ì œì™¸)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    """Lake Formation FGAC ì§€ì› Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName("Operation-SystemAnalysis") \
        .getOrCreate()

def main():
    print("=== Operation Analysis ì‹œì‘ ===")
    print("ì—­í• : ìš´ì˜íŒ€ - ì‹œìŠ¤í…œ ìš´ì˜ ìµœì í™” (ê°œì¸ì •ë³´ ì œì™¸)")
    
    spark = create_spark_session()
    
    try:
        # Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ì½ê¸°
        print("\n1. Lake Formation FGACë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ ì¤‘...")
        namespace = "bike_db"
        table_name = "bike_rental_data"
        
        print(f"   ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
        print(f"   í…Œì´ë¸”ëª…: {table_name}")
        print(f"   ì¹´íƒˆë¡œê·¸: glue_catalog.{namespace}.{table_name}")
        
        # Glue ì¹´íƒˆë¡œê·¸ë¥¼ í†µí•´ ë°ì´í„° ì½ê¸°
        df = spark.read.table(f"glue_catalog.{namespace}.{table_name}")
        
        total_records = df.count()
        print(f"âœ… Lake Formation í•„í„°ë§ í›„ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê±´")
        print("ğŸ”’ ì ‘ê·¼ ì œí•œ: ê°œì¸ì •ë³´ birth_year, gender ì œì™¸")
        
        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ í™•ì¸
        print("\n2. ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„° ìŠ¤í‚¤ë§ˆ:")
        df.printSchema()
        
        # ê¸°ë³¸ ìš´ì˜ í†µê³„
        print(f"\n3. ê¸°ë³¸ ìš´ì˜ í†µê³„:")
        print(f"   â€¢ ì´ ëŒ€ì—¬ ê±´ìˆ˜: {total_records:,}")
        print(f"   â€¢ ê³ ìœ  ëŒ€ì—¬ ID: {df.select('rental_id').distinct().count():,}")
        print(f"   â€¢ ìš´ì˜ ì •ê±°ì¥ ìˆ˜: {df.select('station_id').distinct().count():,}")
        print(f"   â€¢ ì„œë¹„ìŠ¤ êµ¬ì—­ ìˆ˜: {df.select('district').distinct().count()}")
        
        # êµ¬ë³„ ìš´ì˜ í˜„í™©
        print(f"\n4. êµ¬ë³„ ìš´ì˜ í˜„í™©:")
        district_operations = df.groupBy("district") \
                                .agg(count("*").alias("total_rentals"),
                                     countDistinct("station_id").alias("station_count"),
                                     avg("usage_min").alias("avg_usage"),
                                     avg("distance_meter").alias("avg_distance")) \
                                .orderBy(desc("total_rentals"))
        
        district_operations.show(truncate=False)
        
        # ì •ê±°ì¥ë³„ ìš´ì˜ íš¨ìœ¨ì„± (ìƒìœ„ 15ê°œ)
        print(f"\n5. ì •ê±°ì¥ë³„ ìš´ì˜ íš¨ìœ¨ì„± (ìƒìœ„ 15ê°œ):")
        station_efficiency = df.groupBy("station_id", "station_name", "district") \
                               .agg(count("*").alias("total_rentals"),
                                    avg("usage_min").alias("avg_usage_min"),
                                    avg("distance_meter").alias("avg_distance_m"),
                                    min("rental_date").alias("first_rental"),
                                    max("rental_date").alias("last_rental")) \
                               .orderBy(desc("total_rentals")) \
                               .limit(15)
        
        station_efficiency.show(truncate=False)
        
        # ì‹œê°„ëŒ€ë³„ ìš´ì˜ ë¶€í•˜ ë¶„ì„
        print(f"\n6. ì‹œê°„ëŒ€ë³„ ìš´ì˜ ë¶€í•˜ ë¶„ì„:")
        hourly_load = df.withColumn("hour", hour("rental_date")) \
                        .groupBy("hour") \
                        .agg(count("*").alias("rental_count"),
                             avg("usage_min").alias("avg_usage"),
                             countDistinct("station_id").alias("active_stations")) \
                        .orderBy("hour")
        
        hourly_load.show(24)
        
        # ëŒ€ì—¬ ì‹œê°„ ë¶„í¬ (ìš´ì˜ ìµœì í™”ìš©)
        print(f"\n7. ëŒ€ì—¬ ì‹œê°„ ë¶„í¬ (ìš´ì˜ ìµœì í™”ìš©):")
        usage_distribution = df.select(
            avg("usage_min").alias("í‰ê· _ëŒ€ì—¬ì‹œê°„"),
            min("usage_min").alias("ìµœì†Œ_ëŒ€ì—¬ì‹œê°„"),
            max("usage_min").alias("ìµœëŒ€_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.25)").alias("25%_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.5)").alias("ì¤‘ì•™ê°’_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.75)").alias("75%_ëŒ€ì—¬ì‹œê°„"),
            expr("percentile_approx(usage_min, 0.95)").alias("95%_ëŒ€ì—¬ì‹œê°„")
        )
        
        usage_distribution.show()
        
        # ì´ë™ ê±°ë¦¬ ë¶„í¬ (ì„œë¹„ìŠ¤ ë²”ìœ„ ìµœì í™”)
        print(f"\n8. ì´ë™ ê±°ë¦¬ ë¶„í¬ (ì„œë¹„ìŠ¤ ë²”ìœ„ ìµœì í™”):")
        distance_distribution = df.select(
            avg("distance_meter").alias("í‰ê· _ì´ë™ê±°ë¦¬"),
            min("distance_meter").alias("ìµœì†Œ_ì´ë™ê±°ë¦¬"),
            max("distance_meter").alias("ìµœëŒ€_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.25)").alias("25%_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.5)").alias("ì¤‘ì•™ê°’_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.75)").alias("75%_ì´ë™ê±°ë¦¬"),
            expr("percentile_approx(distance_meter, 0.95)").alias("95%_ì´ë™ê±°ë¦¬")
        )
        
        distance_distribution.show()
        
        # ì‚¬ìš©ì ìœ í˜•ë³„ ìš´ì˜ íŒ¨í„´
        print(f"\n9. ì‚¬ìš©ì ìœ í˜•ë³„ ìš´ì˜ íŒ¨í„´:")
        user_type_operations = df.groupBy("user_type") \
                                 .agg(count("*").alias("rental_count"),
                                      avg("usage_min").alias("avg_usage"),
                                      avg("distance_meter").alias("avg_distance"),
                                      countDistinct("station_id").alias("station_usage")) \
                                 .orderBy(desc("rental_count"))
        
        user_type_operations.show()
        
        # ìš”ì¼ë³„ ìš´ì˜ íŒ¨í„´
        print(f"\n10. ìš”ì¼ë³„ ìš´ì˜ íŒ¨í„´:")
        weekday_operations = df.withColumn("weekday", date_format("rental_date", "EEEE")) \
                               .withColumn("day_of_week", dayofweek("rental_date")) \
                               .groupBy("day_of_week", "weekday") \
                               .agg(count("*").alias("rental_count"),
                                    avg("usage_min").alias("avg_usage"),
                                    countDistinct("station_id").alias("active_stations")) \
                               .orderBy("day_of_week")
        
        weekday_operations.show()
        
        # ìš´ì˜ ì´ìƒì¹˜ íƒì§€
        print(f"\n11. ìš´ì˜ ì´ìƒì¹˜ íƒì§€:")
        operational_outliers = df.filter(
            (col("usage_min") > 480) |  # 8ì‹œê°„ ì´ìƒ ëŒ€ì—¬
            (col("distance_meter") > 30000) |  # 30km ì´ìƒ ì´ë™
            (col("usage_min") < 1) |  # 1ë¶„ ë¯¸ë§Œ ëŒ€ì—¬
            (col("distance_meter") < 10)  # 10m ë¯¸ë§Œ ì´ë™
        )
        
        outlier_count = operational_outliers.count()
        print(f"ìš´ì˜ ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count:,}ê±´ ({(outlier_count/total_records)*100:.2f}%)")
        
        if outlier_count > 0:
            print("ìš´ì˜ ì´ìƒì¹˜ ìƒ˜í”Œ (ìƒìœ„ 5ê±´):")
            operational_outliers.select("rental_id", "station_name", "usage_min", "distance_meter", "district") \
                                .orderBy(desc("usage_min")) \
                                .show(5, truncate=False)
        
        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ
        print(f"\n12. ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ:")
        
        # í‰ê·  íšŒì „ìœ¨ (ì •ê±°ì¥ë‹¹ ì¼ì¼ ëŒ€ì—¬ ê±´ìˆ˜)
        daily_turnover = total_records / df.select("station_id").distinct().count()
        print(f"   â€¢ ì •ê±°ì¥ í‰ê·  íšŒì „ìœ¨: {daily_turnover:.1f}ê±´/ì •ê±°ì¥")
        
        # í‰ê·  ì´ìš© íš¨ìœ¨ì„±
        avg_efficiency = df.agg(avg(col("distance_meter") / col("usage_min"))).collect()[0][0]
        print(f"   â€¢ í‰ê·  ì´ìš© íš¨ìœ¨ì„±: {avg_efficiency:.1f}m/ë¶„")
        
        # ì„œë¹„ìŠ¤ ì»¤ë²„ë¦¬ì§€
        total_stations = df.select("station_id").distinct().count()
        active_stations = df.filter(col("usage_min") > 0).select("station_id").distinct().count()
        coverage = (active_stations / total_stations) * 100
        print(f"   â€¢ ì„œë¹„ìŠ¤ ì»¤ë²„ë¦¬ì§€: {coverage:.1f}% ({active_stations}/{total_stations} ì •ê±°ì¥)")
        
        print(f"\n=== Operation Analysis ì™„ë£Œ ===")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {total_records:,}ê±´")
        print(f"ğŸ”’ ê¶Œí•œ: Lake Formation FGAC ì ìš© (ê°œì¸ì •ë³´ birth_year, gender ì œì™¸)")
        print(f"ğŸ“Š ì—­í• : ì‹œìŠ¤í…œ ìš´ì˜ ìµœì í™” ë° ì •ê±°ì¥ ê´€ë¦¬")
        print(f"ğŸ—‚ï¸ ì¹´íƒˆë¡œê·¸: Glue Catalog (Apache Iceberg)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
